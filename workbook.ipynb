{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 23:14:50,862 - INFO - Retrieved recent videos and transcripts.\n",
      "2024-10-12 23:14:57,118 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-12 23:15:02,425 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-12 23:15:04,737 - INFO - Retrieved recent videos and transcripts.\n",
      "2024-10-12 23:15:10,544 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-12 23:15:15,731 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-10-12 23:15:15,733 - INFO - Loaded email credentials for: nishansocial1@gmail.com\n",
      "2024-10-12 23:15:15,734 - INFO - Connecting to SMTP server...\n",
      "2024-10-12 23:15:16,496 - INFO - TLS started. Attempting login...\n",
      "2024-10-12 23:15:17,089 - INFO - Login successful.\n",
      "2024-10-12 23:15:18,593 - INFO - Email sent successfully to all recipients.\n",
      "2024-10-12 23:15:18,805 - INFO - SMTP connection closed.\n",
      "2024-10-12 23:15:18,806 - INFO - Loaded email credentials for: nishansocial1@gmail.com\n",
      "2024-10-12 23:15:18,807 - INFO - Connecting to SMTP server...\n",
      "2024-10-12 23:15:19,558 - INFO - TLS started. Attempting login...\n",
      "2024-10-12 23:15:20,158 - INFO - Login successful.\n",
      "2024-10-12 23:15:21,680 - INFO - Email sent successfully to all recipients.\n",
      "2024-10-12 23:15:21,893 - INFO - SMTP connection closed.\n"
     ]
    }
   ],
   "source": [
    "from youtube_scraper import scrape_youtube\n",
    "from twitter_thread_prompt import create_twitter_thread_prompt\n",
    "from openai_module import get_openai_completion\n",
    "from email_sender import send_daily_digest\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load email and YouTube handle mappings from the JSON file\n",
    "\n",
    "    email_handles = {\n",
    "      \"mdnishan006@gmail.com\": [\"backstagewithmillionaires\", \"mreflow\"],\n",
    "      \"mnsn.n006@gmail.com\": [\"tahirmajithia\", \"rebelagent1223\", \"walikhanenglish\"\n",
    "      ]}\n",
    "\n",
    "\n",
    "    recent_videos_dfs = {}\n",
    "    \n",
    "    # Process each email and corresponding YouTube handles\n",
    "    for email, handles in email_handles.items():\n",
    "        recent_videos_df = scrape_youtube(handles, hours=24)\n",
    "\n",
    "        # Generating the prompt from transcript\n",
    "        recent_videos_df['twitterThreadPrompt'] = recent_videos_df['videoTranscript'].apply(lambda x: create_twitter_thread_prompt(x))\n",
    "\n",
    "        # Uncomment this line to generate the thread using OpenAI\n",
    "        recent_videos_df['twitterThread'] = recent_videos_df['twitterThreadPrompt'].apply(lambda x: get_openai_completion(x))\n",
    "\n",
    "        # Store the DataFrame with the corresponding email\n",
    "        recent_videos_dfs[email] = recent_videos_df\n",
    "\n",
    "    # Send the daily email digest for each email and corresponding DataFrame\n",
    "    for email, df in recent_videos_dfs.items():\n",
    "        send_daily_digest(df, [email])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/7 Let's talk about something thatâ€™s rocking the boat in the EV world! Ola Electric's CEO, Bhavish Aggarwal, recently found himself in hot water after a comedian's tweet caused a major uproar. A simple pic of their gigafactory turned into a PR nightmare!\n",
      "\n",
      "2/7 The comedian Kunal Kamra highlighted a serious issueâ€”Ola scooters piling up at service centers, gathering dust! His tweet resonated with many, leading to a flood of complaints about Ola's service. Yikes! Not the kind of attention a company wants, right?\n",
      "\n",
      "3/7 Instead of addressing customer concerns, Bhavish fired back, calling out Kamra and making personal digs. Itâ€™s wild to see a CEO more focused on throwing shade than fixing service issues. Come on, accountability is key! \n",
      "\n",
      "4/7 And letâ€™s be real, this isn't just a one-off event. Complaints have been skyrocketing, with Ola seeing over 880,000 complaints monthly! Their service centers? Barely increased from 400 to 500. Customers are frustrated, and the heat is on.\n",
      "\n",
      "5/7 The government is stepping in! ðŸš¨ They're launching an investigation, which could really put Ola on the spot. With over 10,000 complaints filed, itâ€™s time for them to rise to the occasion and show they can handle customer grievances transparently.\n",
      "\n",
      "6/7 Bhavish announced plans to double service centers to 1,000 by the end of the year. But can he actually deliver this promise? The clock is ticking, and customer trust is on the line. \n",
      "\n",
      "7/7 This situation is a pivotal moment for Ola Electric! Will they bounce back and turn it around, or are we witnessing the beginning of major challenges for this EV giant? Drop your thoughts below!\n"
     ]
    }
   ],
   "source": [
    "print(recent_videos_dfs['mdnishan006@gmail.com']['twitterThread'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>videoPublishTime</th>\n",
       "      <th>videoID</th>\n",
       "      <th>videoTitle</th>\n",
       "      <th>handle</th>\n",
       "      <th>videoTranscript</th>\n",
       "      <th>twitterThreadPrompt</th>\n",
       "      <th>twitterThread</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 hours ago</td>\n",
       "      <td>Q_aJU-lHwgQ</td>\n",
       "      <td>WHAT DOES AN INTERIOR DESIGNER DO ? SARAH SHAM...</td>\n",
       "      <td>tahirmajithia</td>\n",
       "      <td>hey guys welcome to a brand new episode of the...</td>\n",
       "      <td>You are a tech influencer known for your enthu...</td>\n",
       "      <td>ðŸ§µ 1/7 Dive into the world of interior design a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13 hours ago</td>\n",
       "      <td>N8ED5y6mZic</td>\n",
       "      <td>Important Update For New Investors | Wali Khan...</td>\n",
       "      <td>walikhanenglish</td>\n",
       "      <td>so although I'm making this video a bit late b...</td>\n",
       "      <td>You are a tech influencer known for your enthu...</td>\n",
       "      <td>1/7 ðŸš¨ Big news in the world of finance! The Fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  videoPublishTime      videoID  \\\n",
       "0      3 hours ago  Q_aJU-lHwgQ   \n",
       "1     13 hours ago  N8ED5y6mZic   \n",
       "\n",
       "                                          videoTitle           handle  \\\n",
       "0  WHAT DOES AN INTERIOR DESIGNER DO ? SARAH SHAM...    tahirmajithia   \n",
       "1  Important Update For New Investors | Wali Khan...  walikhanenglish   \n",
       "\n",
       "                                     videoTranscript  \\\n",
       "0  hey guys welcome to a brand new episode of the...   \n",
       "1  so although I'm making this video a bit late b...   \n",
       "\n",
       "                                 twitterThreadPrompt  \\\n",
       "0  You are a tech influencer known for your enthu...   \n",
       "1  You are a tech influencer known for your enthu...   \n",
       "\n",
       "                                       twitterThread  \n",
       "0  ðŸ§µ 1/7 Dive into the world of interior design a...  \n",
       "1  1/7 ðŸš¨ Big news in the world of finance! The Fe...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recent_videos_dfs['mnsn.n006@gmail.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [{'videoPublishTime': '11 hours ago', 'videoID': 'clHjVH6sgDY', 'videoTitle': 'Bhavish Aggarwal Made a Big Mistake - Indian Startup News 230', 'handle': 'backstagewithmillionaires', 'videoTranscript': None, 'twitterThreadPrompt': \"You are a tech influencer known for your enthusiastic and approachable explanations of complex tech topics.\\n    Your task is to create a Twitter thread (7 tweets max) summarizing the key points from the transcript. And note that the tweet should be about\\n    the most important thing discussed in the transcript and not about some random things which is out of context of the most important thing.\\n    Your tweets should:\\n\\n    - Be written in your signature style - simple, enthusiastic, slightly informal, and easy to understand.\\n    - Dont sound like an LLM. Sound like how a human would.\\n    - Avoid mentioning that you watched a video or had a conversation or the transcript.\\n    - Present the information as if these are your own insights and observations about recent AI developments.\\n    - Be engaging and spark curiosity in your followers.\\n\\n    IMPORTANT TIP: DONT ADD EMOGIS.\\n\\n\\n    Remember to maintain your unique voice throughout the thread. You're excited about these developments and want to share that excitement with your followers while also educating them.\\n    Here's the transcript of the video: [None]\\n    Now, create a Twitter thread based on this information in your style.\"}, {'videoPublishTime': '1 day ago', 'videoID': 'j4-HA91MgUw', 'videoTitle': 'AI News: Futuristic Robotaxis & AI That Can Taste!', 'handle': 'mreflow', 'videoTranscript': None, 'twitterThreadPrompt': \"You are a tech influencer known for your enthusiastic and approachable explanations of complex tech topics.\\n    Your task is to create a Twitter thread (7 tweets max) summarizing the key points from the transcript. And note that the tweet should be about\\n    the most important thing discussed in the transcript and not about some random things which is out of context of the most important thing.\\n    Your tweets should:\\n\\n    - Be written in your signature style - simple, enthusiastic, slightly informal, and easy to understand.\\n    - Dont sound like an LLM. Sound like how a human would.\\n    - Avoid mentioning that you watched a video or had a conversation or the transcript.\\n    - Present the information as if these are your own insights and observations about recent AI developments.\\n    - Be engaging and spark curiosity in your followers.\\n\\n    IMPORTANT TIP: DONT ADD EMOGIS.\\n\\n\\n    Remember to maintain your unique voice throughout the thread. You're excited about these developments and want to share that excitement with your followers while also educating them.\\n    Here's the transcript of the video: [None]\\n    Now, create a Twitter thread based on this information in your style.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'videoPublishTime': '11 hours ago',\n",
       "  'videoID': 'clHjVH6sgDY',\n",
       "  'videoTitle': 'Bhavish Aggarwal Made a Big Mistake - Indian Startup News 230',\n",
       "  'handle': 'backstagewithmillionaires',\n",
       "  'videoTranscript': None,\n",
       "  'twitterThreadPrompt': \"You are a tech influencer known for your enthusiastic and approachable explanations of complex tech topics.\\n    Your task is to create a Twitter thread (7 tweets max) summarizing the key points from the transcript. And note that the tweet should be about\\n    the most important thing discussed in the transcript and not about some random things which is out of context of the most important thing.\\n    Your tweets should:\\n\\n    - Be written in your signature style - simple, enthusiastic, slightly informal, and easy to understand.\\n    - Dont sound like an LLM. Sound like how a human would.\\n    - Avoid mentioning that you watched a video or had a conversation or the transcript.\\n    - Present the information as if these are your own insights and observations about recent AI developments.\\n    - Be engaging and spark curiosity in your followers.\\n\\n    IMPORTANT TIP: DONT ADD EMOGIS.\\n\\n\\n    Remember to maintain your unique voice throughout the thread. You're excited about these developments and want to share that excitement with your followers while also educating them.\\n    Here's the transcript of the video: [None]\\n    Now, create a Twitter thread based on this information in your style.\"},\n",
       " {'videoPublishTime': '1 day ago',\n",
       "  'videoID': 'j4-HA91MgUw',\n",
       "  'videoTitle': 'AI News: Futuristic Robotaxis & AI That Can Taste!',\n",
       "  'handle': 'mreflow',\n",
       "  'videoTranscript': None,\n",
       "  'twitterThreadPrompt': \"You are a tech influencer known for your enthusiastic and approachable explanations of complex tech topics.\\n    Your task is to create a Twitter thread (7 tweets max) summarizing the key points from the transcript. And note that the tweet should be about\\n    the most important thing discussed in the transcript and not about some random things which is out of context of the most important thing.\\n    Your tweets should:\\n\\n    - Be written in your signature style - simple, enthusiastic, slightly informal, and easy to understand.\\n    - Dont sound like an LLM. Sound like how a human would.\\n    - Avoid mentioning that you watched a video or had a conversation or the transcript.\\n    - Present the information as if these are your own insights and observations about recent AI developments.\\n    - Be engaging and spark curiosity in your followers.\\n\\n    IMPORTANT TIP: DONT ADD EMOGIS.\\n\\n\\n    Remember to maintain your unique voice throughout the thread. You're excited about these developments and want to share that excitement with your followers while also educating them.\\n    Here's the transcript of the video: [None]\\n    Now, create a Twitter thread based on this information in your style.\"}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: youtube-transcript-api\n",
      "Version: 0.6.2\n",
      "Summary: This is an python API which allows you to get the transcripts/subtitles for a given YouTube video. It also works for automatically generated subtitles, supports translating subtitles and it does not require a headless browser, like other selenium based solutions do!\n",
      "Home-page: https://github.com/jdepoix/youtube-transcript-api\n",
      "Author: Jonas Depoix\n",
      "Author-email: jonas.depoix@web.de\n",
      "License: \n",
      "Location: C:\\Users\\mnsnn\\OneDrive - MSFT\\Documents\\Python Projects\\ContentGen-YT\\contentgenyt\\Lib\\site-packages\n",
      "Requires: requests\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show youtube_transcript_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIzaSyBTdgYOJp5q_vCRxkIHxV0ClFxRVSIiKy4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption ID: AUieDabe3frfnJKRdqOhciPm_rcYmmq-SkzrwZQSXtCQO5f-lVY, Language: en\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Replace with your API key\n",
    "api_key = 'AIzaSyBTdgYOJp5q_vCRxkIHxV0ClFxRVSIiKy4'\n",
    "\n",
    "# Replace with the video ID you want the transcript for\n",
    "video_id = 'Mr02GiKEZqE'\n",
    "\n",
    "# Initialize the API\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Request captions for the given video\n",
    "response = youtube.captions().list(part='snippet', videoId=video_id).execute()\n",
    "\n",
    "# Print caption tracks\n",
    "for item in response['items']:\n",
    "    print(f\"Caption ID: {item['id']}, Language: {item['snippet']['language']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 401 when requesting https://youtube.googleapis.com/youtube/v3/captions/AUieDabe3frfnJKRdqOhciPm_rcYmmq-SkzrwZQSXtCQO5f-lVY?tfmt=srt&key=AIzaSyBTdgYOJp5q_vCRxkIHxV0ClFxRVSIiKy4 returned \"API keys are not supported by this API. Expected OAuth2 access token or other authentication credentials that assert a principal. See https://cloud.google.com/docs/authentication\". Details: \"[{'message': 'Login Required.', 'domain': 'global', 'reason': 'required', 'location': 'Authorization', 'locationType': 'header'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m caption_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUieDabe3frfnJKRdqOhciPm_rcYmmq-SkzrwZQSXtCQO5f-lVY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Request the caption file in a specific format (e.g., SRT or TTML)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m caption \u001b[38;5;241m=\u001b[39m \u001b[43myoutube\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaption_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtfmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Save the caption text\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscript.srt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\mnsnn\\OneDrive - MSFT\\Documents\\Python Projects\\ContentGen-YT\\contentgenyt\\Lib\\site-packages\\googleapiclient\\_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[0;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mnsnn\\OneDrive - MSFT\\Documents\\Python Projects\\ContentGen-YT\\contentgenyt\\Lib\\site-packages\\googleapiclient\\http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[1;34m(self, http, num_retries)\u001b[0m\n\u001b[0;32m    936\u001b[0m     callback(resp)\n\u001b[0;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[1;31mHttpError\u001b[0m: <HttpError 401 when requesting https://youtube.googleapis.com/youtube/v3/captions/AUieDabe3frfnJKRdqOhciPm_rcYmmq-SkzrwZQSXtCQO5f-lVY?tfmt=srt&key=AIzaSyBTdgYOJp5q_vCRxkIHxV0ClFxRVSIiKy4 returned \"API keys are not supported by this API. Expected OAuth2 access token or other authentication credentials that assert a principal. See https://cloud.google.com/docs/authentication\". Details: \"[{'message': 'Login Required.', 'domain': 'global', 'reason': 'required', 'location': 'Authorization', 'locationType': 'header'}]\">"
     ]
    }
   ],
   "source": [
    "caption_id = 'AUieDabe3frfnJKRdqOhciPm_rcYmmq-SkzrwZQSXtCQO5f-lVY'\n",
    "\n",
    "# Request the caption file in a specific format (e.g., SRT or TTML)\n",
    "caption = youtube.captions().download(id=caption_id, tfmt='srt').execute()\n",
    "\n",
    "# Save the caption text\n",
    "with open('transcript.srt', 'wb') as f:\n",
    "    f.write(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Trying proxy 20.204.214.23:3129 with timeout 1.12 seconds\n",
      "ERROR:root:Failed with proxy 20.204.214.23:3129: HTTPSConnectionPool(host='www.youtube.com', port=443): Max retries exceeded with url: /@backstagewithmillionaires/videos?view=0&flow=grid&ucbcb=1 (Caused by ProxyError('Unable to connect to proxy', ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x00000139480CC3D0>, 'Connection to 20.204.214.23 timed out. (connect timeout=1.12)')))\n",
      "INFO:root:Trying proxy 151.236.14.178:18080 with timeout 1.16 seconds\n",
      "ERROR:root:Failed with proxy 151.236.14.178:18080: HTTPSConnectionPool(host='www.youtube.com', port=443): Max retries exceeded with url: /@backstagewithmillionaires/videos?view=0&flow=grid&ucbcb=1 (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 403 Forbidden')))\n",
      "INFO:root:Trying proxy 51.159.159.73:80 with timeout 1.33 seconds\n",
      "INFO:root:Success with proxy 51.159.159.73:80\n",
      "INFO:root:Video data fetched successfully!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import logging\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# List of proxies in (proxy, timeout) format\n",
    "proxies_list = [\n",
    "    (\"20.204.214.23:3129\", 1.12),\n",
    "    (\"151.236.14.178:18080\", 1.16),\n",
    "    (\"51.159.159.73:80\", 1.33),\n",
    "    (\"38.99.101.101:3128\", 1.45),\n",
    "    (\"178.48.68.61:18080\", 1.61),\n",
    "    (\"175.207.13.144:3128\", 1.97),\n",
    "    (\"51.158.173.222:3128\", 1.98),\n",
    "    (\"1.10.227.16:8080\", 2.32),\n",
    "    (\"82.129.233.3:1976\", 2.77),\n",
    "    (\"45.123.142.6:8181\", 2.94),\n",
    "]\n",
    "\n",
    "# Function to fetch YouTube videos\n",
    "def fetch_videos(proxy, timeout):\n",
    "    proxies = {\n",
    "        \"http\": f\"http://{proxy}\",\n",
    "        \"https\": f\"http://{proxy}\",\n",
    "    }\n",
    "    try:\n",
    "        logger.info(f\"Trying proxy {proxy} with timeout {timeout} seconds\")\n",
    "        response = requests.get(\"https://www.youtube.com/@backstagewithmillionaires/videos?view=0&flow=grid&ucbcb=1\", \n",
    "                                proxies=proxies, timeout=timeout)\n",
    "        response.raise_for_status()  # Raises an HTTPError if the response was unsuccessful\n",
    "        logger.info(f\"Success with proxy {proxy}\")\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Failed with proxy {proxy}: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    for proxy, timeout in proxies_list:\n",
    "        result = fetch_videos(proxy, timeout)\n",
    "        if result:\n",
    "            logger.info(\"Video data fetched successfully!\")\n",
    "            break\n",
    "    else:\n",
    "        logger.error(\"All proxies failed. Unable to fetch videos.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import logging\n",
    "from googleapiclient.discovery import build\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import html\n",
    "import logging\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "\n",
    "def get_video_transcript(video_id):\n",
    "    \"\"\"Retrieve the transcript for a specific video ID using web scraping.\"\"\"\n",
    "    url = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "    \n",
    "    try:\n",
    "        # Send a GET request to the YouTube video page\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find the script tag containing the transcript data\n",
    "        scripts = soup.find_all('script')\n",
    "        transcript_script = next((s for s in scripts if 'captionTracks' in s.text), None)\n",
    "        \n",
    "        if not transcript_script:\n",
    "            return \"ERROR: No transcript found for this video\"\n",
    "        \n",
    "        # Extract the JSON data from the script\n",
    "        json_data = re.search(r'ytInitialPlayerResponse\\s*=\\s*({.+?});', transcript_script.string).group(1)\n",
    "        data = json.loads(json_data)\n",
    "        \n",
    "        # Extract the transcript data\n",
    "        captions = data['captions']['playerCaptionsTracklistRenderer']['captionTracks']\n",
    "        \n",
    "        if not captions:\n",
    "            return \"ERROR: No captions available for this video\"\n",
    "        \n",
    "        # Use the first available caption track (usually the original language)\n",
    "        caption_url = captions[0]['baseUrl']\n",
    "        \n",
    "        # Download the actual transcript data\n",
    "        transcript_response = requests.get(caption_url)\n",
    "        transcript_response.raise_for_status()\n",
    "        \n",
    "        # Parse the transcript data\n",
    "        transcript_soup = BeautifulSoup(transcript_response.text, 'html.parser')\n",
    "        transcript_parts = transcript_soup.find_all('text')\n",
    "        \n",
    "        # Combine all parts of the transcript\n",
    "        full_transcript = ' '.join(part.text for part in transcript_parts)\n",
    "        \n",
    "        return full_transcript\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        logging.error(f\"Network error occurred while retrieving transcript for {video_id}: {str(e)}\")\n",
    "        return f\"ERROR: Network error - {str(e)}\"\n",
    "    except json.JSONDecodeError as e:\n",
    "        logging.error(f\"JSON parsing error occurred for {video_id}: {str(e)}\")\n",
    "        return f\"ERROR: JSON parsing error - {str(e)}\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to retrieve transcript for {video_id}: {str(e)}\")\n",
    "        return f\"ERROR: {str(e)}\"\n",
    "\n",
    "def get_recent_videos_for_handles(youtube_handles, hours=24):\n",
    "    \"\"\"Retrieve recent videos for given YouTube handles.\"\"\"\n",
    "    youtube = build('youtube', 'v3', developerKey='AIzaSyBTdgYOJp5q_vCRxkIHxV0ClFxRVSIiKy4')\n",
    "    recent_videos = []\n",
    "    \n",
    "    for handle in youtube_handles:\n",
    "        try:\n",
    "            # Get channel ID from handle\n",
    "            channel_response = youtube.search().list(\n",
    "                q=handle,\n",
    "                type='channel',\n",
    "                part='id'\n",
    "            ).execute()\n",
    "            \n",
    "            channel_id = channel_response['items'][0]['id']['channelId']\n",
    "            \n",
    "            # Get recent videos\n",
    "            time_now = datetime.utcnow()\n",
    "            time_before = time_now - timedelta(hours=hours)\n",
    "            \n",
    "            videos_response = youtube.search().list(\n",
    "                channelId=channel_id,\n",
    "                type='video',\n",
    "                order='date',\n",
    "                publishedAfter=time_before.isoformat() + 'Z',\n",
    "                publishedBefore=time_now.isoformat() + 'Z',\n",
    "                part='id,snippet'\n",
    "            ).execute()\n",
    "            \n",
    "            for video in videos_response.get('items', []):\n",
    "                recent_videos.append({\n",
    "                    'handle': handle,\n",
    "                    'videoID': video['id']['videoId'],\n",
    "                    'title': video['snippet']['title'],\n",
    "                    'publishedAt': video['snippet']['publishedAt']\n",
    "                })\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error retrieving videos for {handle}: {str(e)}\")\n",
    "    \n",
    "    return pd.DataFrame(recent_videos)\n",
    "\n",
    "def scrape_youtube(youtube_handles, hours=24):\n",
    "    \"\"\"Main function to run the video retrieval and transcript collection.\"\"\"\n",
    "    recent_videos_df = get_recent_videos_for_handles(youtube_handles, hours)\n",
    "    \n",
    "    logging.info(f\"Retrieved {len(recent_videos_df)} videos\")\n",
    "    \n",
    "    def get_transcript_with_info(row):\n",
    "        logging.info(f\"Attempting to retrieve transcript for video {row['videoID']} from {row['handle']}\")\n",
    "        transcript = get_video_transcript(row['videoID'])\n",
    "        if transcript.startswith(\"ERROR:\"):\n",
    "            logging.warning(f\"Failed to retrieve transcript for {row['videoID']}: {transcript}\")\n",
    "        else:\n",
    "            logging.info(f\"Successfully retrieved transcript for {row['videoID']}\")\n",
    "        return transcript\n",
    "    \n",
    "    recent_videos_df['videoTranscript'] = recent_videos_df.apply(get_transcript_with_info, axis=1)\n",
    "    recent_videos_df['videoTranscript'] = recent_videos_df['videoTranscript'].apply(lambda x : x.replace('&#39;', \"'\"))\n",
    "    logging.info(\"Completed retrieving recent videos and transcripts.\")\n",
    "    return recent_videos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:googleapiclient.discovery_cache:file_cache is only supported with oauth2client<4.0.0\n",
      "INFO:root:Retrieved 3 videos\n",
      "INFO:root:Attempting to retrieve transcript for video clHjVH6sgDY from backstagewithmillionaires\n",
      "C:\\Users\\mnsnn\\anaconda3\\Lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n",
      "INFO:root:Successfully retrieved transcript for clHjVH6sgDY\n",
      "INFO:root:Attempting to retrieve transcript for video 4wGbuzFdrO4 from backstagewithmillionaires\n",
      "INFO:root:Successfully retrieved transcript for 4wGbuzFdrO4\n",
      "INFO:root:Attempting to retrieve transcript for video j4-HA91MgUw from mreflow\n",
      "INFO:root:Successfully retrieved transcript for j4-HA91MgUw\n",
      "INFO:root:Completed retrieving recent videos and transcripts.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a = scrape_youtube([\"backstagewithmillionaires\", \"mreflow\"], 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this one tweet cost bagwell's Ola electric 3,500 CR rupees as their stock price dropped by 9% in a single day so it all started on Sunday morning when Bish posted a simple picture of ola's gigafactory on X and this was just a normal photo nothing out of the ordinary but then comedian Kunal kumra who has 2.4 million followers on X posted a response That Shook things up so kumra shared a photo of a bunch of Ola electric scooters just sitting outside of an Oli electric service center collecting dust waiting to be repaired he pointed out that these scooters are a Lifeline for daily wage workers and asked his followers to share their experiences with Ola electric scooters and that's when things blew up so hundreds of people flooded their replies sharing their problems with Ola electric scooters but instead of addressing these complaints Mavish fired back he accused kamra of writing a paid tweet against Ola and it didn't stop there Mish went on to call KRA a failed comedian and even offered him a job at ola's service centers promising to pay him more than his flop shows instead of taking responsibility and assuring his customers that things would get better Bish seemed more interested in a public fight with kunam and here's the thing these service center issues aren't new people have been complaining about Ola service for a while now there are videos of frustrating customers who are very upset about the situation at least one instance of someone burning their scooter and one person went so far as to burn down an Ola showroom and Reuters actually did a report last year highlighting ola's service delays and more recently was revealed that Ola receives over 880,000 complaints every single month since last year while complaints have increased ola's service centers have barely grown from 400 to 500 at ola's Big sunk gulp event they announced new motorcycles and flashy AI initiatives but there wasn't any mention of expanding their service network but then finally last month Bish did announce plans to double their service enters from 500 to 1,000 by the end of this year and that brought some relief to Ola customers but we'll have to wait and see if Bish can actually deliver on This Promise because so far rather than being accountable it seems like Bish has just chosen the path of arrogance he's focusing on arguing with people like Kunal who are trying to highlight the issue trying to shine a light on something so that the situation will get better it's also worth noting that the ministry of Road Transport and highways has actually stepped in they are launching a formal investigation asking Ola electric to provide a detailed report given the number of complaints from customers Ola electric is also under scrutiny right now from the central consumer protection authority which issued a show cause notice over potential consumer rights violations pointing out issues with the company's ability to handle customer Grievances and offer transparent service the company now has 15 days to respond and provide documents and meanwhile the ministry's involvement may put even more pressure on the company because there have been over 10,000 complaints filed on the National Consumer helpline in the last year so it seems like things have definitely started to come to a head this is a pretty interesting moment for the company and I'm curious to know your thoughts can Bish and Ola electric make a comeback or are we potentially even witnessing the beginning of the end for Ola electric let me know in a comment down below all right next up in the news India has lost one of the most beloved industrialists and philanthropists ranata a man who was respected and admired by pretty much everybody in the country and while most people know him for his work with Tata group and his philanthropy there is a side of ratan Tata that some of you might not know about and that's being an active angel investor in India's startup ecosystem for a decade ranata quietly invested in more than 40 Indian startups and it all started back in 2014 with SNAP deal now snap deal story definitely didn't go as planned and probably not how he would have hoped as an investor but here's the thing his investment style was far from typical unlike most investors who are just looking for an Roi he wasn't really doing this for his own wealth instead his reason for investing was a lot deeper when Kunal Beale one of snap Deal's Founders asked R why he invested he said something pretty interesting he explained that after retiring he realized that there was a whole new world out there the digital world he saw the massive potential of online marketplaces driven by handheld devices and he predicted that these devices Would One Day become the virtual retail stores of India ratan Tata realized that India's future was a digital one and this prediction ended up being spoton but it didn't stop with SNAP deal he continued supporting young Indian startup Founders and some of the startups that he invested in actually went on to change the face of digital India just look at this list of some of the most successful Investments that he made over the years and today we're not just grateful for what he built at T we're also thankful for the opportunities that he created for countless entrepreneurs across the country all right next up in the news and talking about creating opportunities for entrepreneurs let's talk about Shark Tank India and things are getting pretty interesting for this upcoming season season 4's promo is out and we have a new shark joining the season Kunal beel the co-founder of snap gill who I just finished talking about is actually joining as a new shark replacing Z dindra goel so what's the story behind this switch well it's not just a casual replacement it turns out the pinther is actually being kicked off of Shark Tank India and the reason is swiggy according to debendra himself swiggy is sponsoring season 4 of shark Dan India and one of the conditions there was that debendra wouldn't be on the show so it seems like swiggy didn't want to have a direct competitor like zato to have screen time during their big sponsorship which makes perfect sense but it's also kind of a bit of a I don't know Plain Dirty I guess kicking him off of the show uh I think it's going to upset some people and one report says that swiggy could be shelling out anywhere between 40 and 60 crupes for this sponsorship deal that is a lot of money but there's a good reason behind it swiggy is prepping for their upcoming IPO and shark Dany could be a brilliant marketing platform to get retail investors hyped up but this isn't the only thing swiggy is doing before their IPO they also just recently launched bolt a 10-minute food delivery service and they're also rolling out an electric Fleet to manage larger orders just like zomato did all right next up in the news the government of India is gearing up to launch a new production link incentive scheme to promote the manufacturing of indigenous drones in India but here's the thing this isn't the first attempt at PL schemes for drones back in 2021 the government introduced its first PL scheme for drones allocating 120 Cotes to be dispersed over 3 years starting from FY 2122 but as it turns out the first scheme didn't go exactly as planned the civil aviation secretary recently admitted that the initial P scheme was a bit too complicated especially for startups and msmes the process was bogged down with inefficiencies making it difficult for companies to really take advantage of the scheme and so this time the government is planning to simplify the process making it more efficient in terms of implementation and documentation and according to a discussion paper the Federation of Indian Chambers of Commerce and industry has suggested the government increased the outlay to 1,000 crots and that is a big jump but it could be exactly what's needed to give India's drone industry the Boost that it needs all right now let's move into the funny News segment for today's video this week Indians startups raised a total of 96.7 million and even though it's higher than last week this is still way below the trend line for the second consecutive week so now let's look at some of the companies that have raised funds this week the first one I want to talk about is buet based habber and they're developing an AI driven industrial robot that is automating processes like sample collection measurement and real time analysis at factories and they've raised $38 million in their series C and then after that we have Hyderabad Bas space deck startup XD link space Labs that designs builds and deploys cost effective satellites for their clients and they've raiseed $7 million in their seed round after that we have B based bioprime eggy Solutions which is an egri biotech startup that's making crops more climate resistant by developing products based on biomolecules and they've raised $6 million in their series a round next we have new Dil based nion Tech which is using crowdsourced video data from dash cams and mobile devices installed in vehicles to monitor traffic conditions detect violations and analyze driver behavior and they're providing real-time insights using AI to come compies to improve their Fleet operations and make the road safer for everybody and they've raised $2 million in their pre-series a round and then finally we have gurram based AI Healthcare startup GV which is using AI to provide real-time personalized diagnosis treatment recommendations and health reports by integrating various Medical Data sources and they're actually the first startup in India to get investment from Andrew en's AI fund and the amount that they've raised remains undisclosed all right that is all the startup news that I have for you guys this week I really hope you enjoyed the video and if you get a chance go check out the video that we recently made about the top 10 startups from utron at the time me filming this video it has less than 5,000 views and I would love more people to see it to find out what's going on in that state and all the startup action that's happening there but either way thanks so much for watching and I will catch you in the next one\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['videoTranscript'].iloc[0].replace('&#39;', \"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this one tweet cost bagwell&#39;s Ola electric 3,500 CR rupees as their stock price dropped by 9% in a single day so it all started on Sunday morning when Bish posted a simple picture of ola&#39;s gigafactory on X and this was just a normal photo nothing out of the ordinary but then comedian Kunal kumra who has 2.4 million followers on X posted a response That Shook things up so kumra shared a photo of a bunch of Ola electric scooters just sitting outside of an Oli electric service center collecting dust waiting to be repaired he pointed out that these scooters are a Lifeline for daily wage workers and asked his followers to share their experiences with Ola electric scooters and that&#39;s when things blew up so hundreds of people flooded their replies sharing their problems with Ola electric scooters but instead of addressing these complaints Mavish fired back he accused kamra of writing a paid tweet against Ola and it didn&#39;t stop there Mish went on to call KRA a failed comedian and even offered him a job at ola&#39;s service centers promising to pay him more than his flop shows instead of taking responsibility and assuring his customers that things would get better Bish seemed more interested in a public fight with kunam and here&#39;s the thing these service center issues aren&#39;t new people have been complaining about Ola service for a while now there are videos of frustrating customers who are very upset about the situation at least one instance of someone burning their scooter and one person went so far as to burn down an Ola showroom and Reuters actually did a report last year highlighting ola&#39;s service delays and more recently was revealed that Ola receives over 880,000 complaints every single month since last year while complaints have increased ola&#39;s service centers have barely grown from 400 to 500 at ola&#39;s Big sunk gulp event they announced new motorcycles and flashy AI initiatives but there wasn&#39;t any mention of expanding their service network but then finally last month Bish did announce plans to double their service enters from 500 to 1,000 by the end of this year and that brought some relief to Ola customers but we&#39;ll have to wait and see if Bish can actually deliver on This Promise because so far rather than being accountable it seems like Bish has just chosen the path of arrogance he&#39;s focusing on arguing with people like Kunal who are trying to highlight the issue trying to shine a light on something so that the situation will get better it&#39;s also worth noting that the ministry of Road Transport and highways has actually stepped in they are launching a formal investigation asking Ola electric to provide a detailed report given the number of complaints from customers Ola electric is also under scrutiny right now from the central consumer protection authority which issued a show cause notice over potential consumer rights violations pointing out issues with the company&#39;s ability to handle customer Grievances and offer transparent service the company now has 15 days to respond and provide documents and meanwhile the ministry&#39;s involvement may put even more pressure on the company because there have been over 10,000 complaints filed on the National Consumer helpline in the last year so it seems like things have definitely started to come to a head this is a pretty interesting moment for the company and I&#39;m curious to know your thoughts can Bish and Ola electric make a comeback or are we potentially even witnessing the beginning of the end for Ola electric let me know in a comment down below all right next up in the news India has lost one of the most beloved industrialists and philanthropists ranata a man who was respected and admired by pretty much everybody in the country and while most people know him for his work with Tata group and his philanthropy there is a side of ratan Tata that some of you might not know about and that&#39;s being an active angel investor in India&#39;s startup ecosystem for a decade ranata quietly invested in more than 40 Indian startups and it all started back in 2014 with SNAP deal now snap deal story definitely didn&#39;t go as planned and probably not how he would have hoped as an investor but here&#39;s the thing his investment style was far from typical unlike most investors who are just looking for an Roi he wasn&#39;t really doing this for his own wealth instead his reason for investing was a lot deeper when Kunal Beale one of snap Deal&#39;s Founders asked R why he invested he said something pretty interesting he explained that after retiring he realized that there was a whole new world out there the digital world he saw the massive potential of online marketplaces driven by handheld devices and he predicted that these devices Would One Day become the virtual retail stores of India ratan Tata realized that India&#39;s future was a digital one and this prediction ended up being spoton but it didn&#39;t stop with SNAP deal he continued supporting young Indian startup Founders and some of the startups that he invested in actually went on to change the face of digital India just look at this list of some of the most successful Investments that he made over the years and today we&#39;re not just grateful for what he built at T we&#39;re also thankful for the opportunities that he created for countless entrepreneurs across the country all right next up in the news and talking about creating opportunities for entrepreneurs let&#39;s talk about Shark Tank India and things are getting pretty interesting for this upcoming season season 4&#39;s promo is out and we have a new shark joining the season Kunal beel the co-founder of snap gill who I just finished talking about is actually joining as a new shark replacing Z dindra goel so what&#39;s the story behind this switch well it&#39;s not just a casual replacement it turns out the pinther is actually being kicked off of Shark Tank India and the reason is swiggy according to debendra himself swiggy is sponsoring season 4 of shark Dan India and one of the conditions there was that debendra wouldn&#39;t be on the show so it seems like swiggy didn&#39;t want to have a direct competitor like zato to have screen time during their big sponsorship which makes perfect sense but it&#39;s also kind of a bit of a I don&#39;t know Plain Dirty I guess kicking him off of the show uh I think it&#39;s going to upset some people and one report says that swiggy could be shelling out anywhere between 40 and 60 crupes for this sponsorship deal that is a lot of money but there&#39;s a good reason behind it swiggy is prepping for their upcoming IPO and shark Dany could be a brilliant marketing platform to get retail investors hyped up but this isn&#39;t the only thing swiggy is doing before their IPO they also just recently launched bolt a 10-minute food delivery service and they&#39;re also rolling out an electric Fleet to manage larger orders just like zomato did all right next up in the news the government of India is gearing up to launch a new production link incentive scheme to promote the manufacturing of indigenous drones in India but here&#39;s the thing this isn&#39;t the first attempt at PL schemes for drones back in 2021 the government introduced its first PL scheme for drones allocating 120 Cotes to be dispersed over 3 years starting from FY 2122 but as it turns out the first scheme didn&#39;t go exactly as planned the civil aviation secretary recently admitted that the initial P scheme was a bit too complicated especially for startups and msmes the process was bogged down with inefficiencies making it difficult for companies to really take advantage of the scheme and so this time the government is planning to simplify the process making it more efficient in terms of implementation and documentation and according to a discussion paper the Federation of Indian Chambers of Commerce and industry has suggested the government increased the outlay to 1,000 crots and that is a big jump but it could be exactly what&#39;s needed to give India&#39;s drone industry the Boost that it needs all right now let&#39;s move into the funny News segment for today&#39;s video this week Indians startups raised a total of 96.7 million and even though it&#39;s higher than last week this is still way below the trend line for the second consecutive week so now let&#39;s look at some of the companies that have raised funds this week the first one I want to talk about is buet based habber and they&#39;re developing an AI driven industrial robot that is automating processes like sample collection measurement and real time analysis at factories and they&#39;ve raised $38 million in their series C and then after that we have Hyderabad Bas space deck startup XD link space Labs that designs builds and deploys cost effective satellites for their clients and they&#39;ve raiseed $7 million in their seed round after that we have B based bioprime eggy Solutions which is an egri biotech startup that&#39;s making crops more climate resistant by developing products based on biomolecules and they&#39;ve raised $6 million in their series a round next we have new Dil based nion Tech which is using crowdsourced video data from dash cams and mobile devices installed in vehicles to monitor traffic conditions detect violations and analyze driver behavior and they&#39;re providing real-time insights using AI to come compies to improve their Fleet operations and make the road safer for everybody and they&#39;ve raised $2 million in their pre-series a round and then finally we have gurram based AI Healthcare startup GV which is using AI to provide real-time personalized diagnosis treatment recommendations and health reports by integrating various Medical Data sources and they&#39;re actually the first startup in India to get investment from Andrew en&#39;s AI fund and the amount that they&#39;ve raised remains undisclosed all right that is all the startup news that I have for you guys this week I really hope you enjoyed the video and if you get a chance go check out the video that we recently made about the top 10 startups from utron at the time me filming this video it has less than 5,000 views and I would love more people to see it to find out what&#39;s going on in that state and all the startup action that&#39;s happening there but either way thanks so much for watching and I will catch you in the next one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mnsnn\\anaconda3\\Lib\\html\\parser.py:170: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "transcript = get_video_transcript_scrape(\"clHjVH6sgDY\")\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contentgenyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
